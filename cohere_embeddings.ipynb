{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk over markdown files in ./memory, extract links, and store in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[39m# Loop through each markdown file in the memory folder\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39m./memory\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.md\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[39m# Read the file text\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m./memory\u001b[39m\u001b[39m\"\u001b[39m, filename), \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './memory'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define a function to extract tags and links from markdown file text\n",
    "def extract_tags_links(text):\n",
    "    links = re.findall(r\"\\[\\[.*?\\]\\]\", text)\n",
    "    links = [link.strip(\"[[\").strip(\"]]\") for link in links]\n",
    "    return links\n",
    "\n",
    "# Create an empty list to store the results\n",
    "data = []\n",
    "\n",
    "# Loop through each markdown file in the memory folder\n",
    "for filename in os.listdir(\"./data/memory\"):\n",
    "    if filename.endswith(\".md\"):\n",
    "        # Read the file text\n",
    "        with open(os.path.join(\"./data/memory\", filename), \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Extract the tags and links from the text\n",
    "        links = extract_tags_links(text)\n",
    "\n",
    "        # Add a dictionary to the list, removed md extension from filename\n",
    "        data.append({\"filename\": filename[:-3], \"text\": text, \"links\": links})\n",
    "\n",
    "# Convert the list of dictionaries to a dataframe\n",
    "df = pd.DataFrame(data, columns=[\"filename\", \"text\", \"links\"])\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv(\"./data/memory_dataframe.csv\", index=False)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break up text into 512 token chunks and batches of length 96 - required by cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import time\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# create a ConfigParser object and read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# get the API key from the configuration file\n",
    "api_key = config.get('cohere', 'api_key')\n",
    "\n",
    "def chunk_text(text, max_len=512):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(chunk) + len(word) + 1 <= max_len:\n",
    "            chunk.append(word)\n",
    "        else:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = [word]\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Assuming df is the dataframe with columns 'filename' and 'text'\n",
    "# and api_key is your cohere API key\n",
    "\n",
    "co = cohere.Client(api_key)\n",
    "embeddings = []\n",
    "\n",
    "batch_size = 96\n",
    "rate_limit_calls = 100\n",
    "rate_limit_duration = 60\n",
    "\n",
    "start_time = time.time()\n",
    "call_count = 0\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text_chunks = chunk_text(row['text'])\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        embeddings.append({\n",
    "            'filename': row['filename'],\n",
    "            'index': i,\n",
    "            'chunk_text': chunk,\n",
    "            'embedding': None\n",
    "        })\n",
    "\n",
    "embedding_batches = [embeddings[i:i+batch_size] for i in range(0, len(embeddings), batch_size)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed all files and store in a csv (files broken into chunks have filename, index (1,2,n) in the column with a separate embedding for each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in embedding_batches:\n",
    "    texts = [item['chunk_text'] for item in batch]\n",
    "    response = co.embed(texts=texts, model='large', truncate='END')\n",
    "    \n",
    "    for i, embedding in enumerate(response.embeddings):\n",
    "        batch[i]['embedding'] = embedding\n",
    "\n",
    "    call_count += 1\n",
    "\n",
    "    if call_count >= rate_limit_calls:\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if elapsed_time < rate_limit_duration:\n",
    "            time.sleep(rate_limit_duration - elapsed_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        call_count = 0\n",
    "\n",
    "df_embeddings = pd.DataFrame(embeddings)\n",
    "\n",
    "# adds links column\n",
    "df_embeddings = df_embeddings.merge(df[['filename', 'links']], on='filename', how='left')\n",
    "df_embeddings.to_csv('memory_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
